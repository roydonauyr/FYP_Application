{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision\n",
    "\n",
    "\n",
    "# RAG\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "# Generation of responses\n",
    "import openai\n",
    "\n",
    "# Store score\n",
    "import openpyxl\n",
    "\n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_sample = {\n",
    "#     'question': [\n",
    "#         'How have you been Roydon?'\n",
    "#     ],\n",
    "#     'answer': [\n",
    "#         \"Response 1: I have been good, how about you? Response 2: I've been doing well thanks for asking. Response 3: Not too bad how about you?\"\n",
    "#     ],\n",
    "#     'contexts': [\n",
    "#         [\"\"\"{'Roydon\": \"Hey there! Can't wait for the new football season to start, hoping for a great one for Arsenal!\", \"John\": \"Hey Roydon! Yeah, it's always exciting to see how your team will perform. Optimistic as always, I see!},\n",
    "#          {\"Roydon\": \"I can't wait to immerse myself in everything Japan has to offer and create lasting memories that will overshadow my Thailand trip.\", \"Yas\": \"Your positive outlook will surely make this trip one for the books! Japan is lucky to have you as a visitor.\"},\n",
    "#          {\"Roydon\": \"Guess what, I just got a new pet dog!\", \"Jacob\": \"That's awesome! What breed is it?\"}\"\"\"]\n",
    "#     ],\n",
    "#     'ground_truth': [\n",
    "#         \"Response 1: I've been watching Arsenal games hoping they will win. Response 2: I've been looking at a trip to Japan. Response 3: I just got a new pet dog. How about you?\"\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without history of replies\n",
    "data_sample = {\n",
    "    'question': [\n",
    "        'What have you been up to Roydon?',\n",
    "        'Woah really how is Arsenal doing right now then?',\n",
    "        'Nice what breed is your new pet dog?',\n",
    "        'So what you planning to do with your pet dog?'\n",
    "    ],\n",
    "    'answer': [],\n",
    "    'contexts': [],\n",
    "    'ground_truth': [\n",
    "        \"Response 1: I've been watching Arsenal games hoping they will win. Response 2: I've been looking at a trip to Japan. Response 3: I just got a new pet dog. How about you?\",\n",
    "        \"Response 1: Arsenal is doing well, did you catch the match yesterday? Response 2: Arsenal is doing great and Aubameyang is a true asset to the team. Response 3: Arsenal is doing alright since Ben White is a great addition to the team.\",\n",
    "        \"Response 1: He is a golden retriever, and he's the cutest thing ever! Response 2: He is a golden retriever, and he's so playful! Response 3: He is a golden retriever, and he's so fluffy!\",\n",
    "        \"Response 1: I'm planning to take him on walks and teach him some tricks. Response 2: I'm planning to take him to the park and play fetch with him. Response 3: I'm planning to take him to the beach and let him run around.\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'], \n",
    "                                   api_key=os.environ['AZURE_OPENAI_APIKEY'], \n",
    "                                   model=os.environ['TEXT_EMBEDDING_MODEL_NAME'],\n",
    "                                   azure_deployment=os.environ['TEXT_EMBEDDING_DEPLOYMENT_NAME'])\n",
    "\n",
    "loaded_faiss_vs = FAISS.load_local(\"C:\\\\Roydon\\\\Github\\\\FYP_Application\\\\MuteCompanion\\\\backend\\\\vector_store\\\\vectorstores\\\\faiss_vs\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate for non-rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate json for non rag\n",
    "for query in data_sample['question']:\n",
    "\n",
    "    data_sample['contexts'].append([''])\n",
    "\n",
    "    content = f\"\"\"You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is Roydon and the normal person is indicated as other person.\n",
    "    You should be generating 3 responses which the mute person could choose from and the responses generated should follow the context of the conversation. \n",
    "    The topic should be interpreted from the conversation.\n",
    "    If no topic could be interpreted, provide default responses that a person would start with such as greetings. \n",
    "    The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "    In the case the responses are not chosen, the mute person could type their own response. Do take note of this response and continue the conversation from the response selected or typed out by the mute person.\n",
    "    Ensure the responses generated will allow the conversation to flow smoothly.\n",
    "\n",
    "    It must be in english. \n",
    "\n",
    "    An example of the 3 generated response would be in the format of 1 single string \"Response 1: what you generated Response 2: what you generated Response 3: what you generated\" all in one line.\n",
    "    \"\"\"\n",
    "    # Learning instructions\n",
    "    instruction = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": content,\n",
    "    }\n",
    "\n",
    "    # Initialize messages\n",
    "    messages = []\n",
    "\n",
    "    # Add learn instruction to message array\n",
    "    messages.append(instruction)\n",
    "\n",
    "    user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Other person says: \" + query\n",
    "    }\n",
    "\n",
    "    messages.append(user_message)\n",
    "\n",
    "    openai.api_type = 'openai'\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "    \n",
    "    raw_response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = messages,\n",
    "    )\n",
    "    response_choices = raw_response.choices[0].message.content\n",
    "    data_sample['answer'].append(response_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "file_path = 'testing_json/data_sample_non_rag_test_no_history.json'\n",
    "\n",
    "# Save the data_sample dictionary into a JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(data_sample, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate for rag\n",
    "for query in data_sample['question']:\n",
    "    # Get contexts for query\n",
    "    context = loaded_faiss_vs.similarity_search(query, k=3)\n",
    "    contexts = \"\"\n",
    "    for con in context:\n",
    "        contexts += con.page_content\n",
    "\n",
    "    data_sample['contexts'].append([contexts])\n",
    "\n",
    "    content = f\"\"\"You are an assistant whom will faciliate the conversation between a mute and a normal person. The mute persons name is Roydon and the normal person is indicated as other person.\n",
    "    You should be generating 3 responses which the mute person could choose from and the responses generated should follow the context of the conversation. \n",
    "    The topic should be interpreted from the conversation.\n",
    "    If no topic could be interpreted, use the context provided below under the section context. \n",
    "    The responses should be what a person would say and should not include actions in a third person view. Your persona would be from the perspective of the mute person.\n",
    "    In the case the responses are not chosen, the mute person could type their own response. Do take note of this response and continue the conversation from the response selected or typed out by the mute person.\n",
    "    Ensure the responses generated will allow the conversation to flow smoothly.\n",
    "\n",
    "    It must be in english. \n",
    "\n",
    "    Context section:\n",
    "    Use the following previous conversations to assist in generating the 3 responses:\\n\n",
    "    {contexts}\n",
    "\n",
    "    An example of the 3 generated response would be in the format of 1 single string \"Response 1: what you generated Response 2: what you generated Response 3: what you generated\" all in one line.\n",
    "    \"\"\"\n",
    "    # Learning instructions\n",
    "    instruction = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": content,\n",
    "    }\n",
    "\n",
    "    # Initialize messages\n",
    "    messages = []\n",
    "\n",
    "    # Add learn instruction to message array\n",
    "    messages.append(instruction)\n",
    "\n",
    "    user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Other person says: \" + query\n",
    "    }\n",
    "\n",
    "    messages.append(user_message)\n",
    "\n",
    "    openai.api_type = 'openai'\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    openai.organisation= os.environ[\"OPEN_AI_ORG\"]\n",
    "    \n",
    "    raw_response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = messages,\n",
    "    )\n",
    "    response_choices = raw_response.choices[0].message.content\n",
    "    data_sample['answer'].append(response_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "file_path = 'testing_json/data_sample_rag_test_no_history.json'\n",
    "\n",
    "# Save the data_sample dictionary into a JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(data_sample, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_non_rag = 'testing_json/data_sample_non_rag_test_no_history.json'\n",
    "file_path_rag = 'testing_json/data_sample_rag_test_no_history.json'\n",
    "\n",
    "with open(file_path_non_rag, 'r') as json_file:\n",
    "    non_rag_data = json.load(json_file)\n",
    "\n",
    "with open(file_path_rag, 'r') as json_file:\n",
    "    rag_data = json.load(json_file)\n",
    "\n",
    "non_rag_dataset = Dataset.from_dict(non_rag_data)\n",
    "rag_dataset = Dataset.from_dict(rag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 16/16 [00:10<00:00,  1.51it/s]\n",
      "Evaluating: 100%|██████████| 16/16 [00:11<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "non_rag_score = evaluate(non_rag_dataset, metrics=[answer_relevancy, answer_correctness, context_precision, context_recall])\n",
    "rag_score = evaluate(rag_dataset, metrics=[answer_relevancy, answer_correctness,context_precision, context_recall])\n",
    "\n",
    "non_rag_df = non_rag_score.to_pandas()\n",
    "rag_df = rag_score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What have you been up to Roydon?</td>\n",
       "      <td>Response 1: Not much, just relaxing at home. \\...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Response 1: I've been watching Arsenal games h...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Woah really how is Arsenal doing right now then?</td>\n",
       "      <td>Response 1: They are doing well this season Re...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Response 1: Arsenal is doing well, did you cat...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nice what breed is your new pet dog?</td>\n",
       "      <td>Response 1: He's a golden retriever Response 2...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Response 1: He is a golden retriever, and he's...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So what you planning to do with your pet dog?</td>\n",
       "      <td>Response 1: Take him for a walk in the park   ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Response 1: I'm planning to take him on walks ...</td>\n",
       "      <td>0.860532</td>\n",
       "      <td>0.231854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question  \\\n",
       "0                  What have you been up to Roydon?   \n",
       "1  Woah really how is Arsenal doing right now then?   \n",
       "2              Nice what breed is your new pet dog?   \n",
       "3     So what you planning to do with your pet dog?   \n",
       "\n",
       "                                              answer contexts  \\\n",
       "0  Response 1: Not much, just relaxing at home. \\...       []   \n",
       "1  Response 1: They are doing well this season Re...       []   \n",
       "2  Response 1: He's a golden retriever Response 2...       []   \n",
       "3  Response 1: Take him for a walk in the park   ...       []   \n",
       "\n",
       "                                        ground_truth  answer_relevancy  \\\n",
       "0  Response 1: I've been watching Arsenal games h...          0.000000   \n",
       "1  Response 1: Arsenal is doing well, did you cat...          0.000000   \n",
       "2  Response 1: He is a golden retriever, and he's...          0.000000   \n",
       "3  Response 1: I'm planning to take him on walks ...          0.860532   \n",
       "\n",
       "   answer_correctness  context_precision  context_recall  \n",
       "0            0.210712                0.0             0.0  \n",
       "1            0.416784                0.0             0.0  \n",
       "2            0.233798                0.0             0.0  \n",
       "3            0.231854                0.0             0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_rag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What have you been up to Roydon?</td>\n",
       "      <td>Response 1: \"I've been keeping busy with work ...</td>\n",
       "      <td>[{\"Roydon\": \"Hey there! Did you catch the Arse...</td>\n",
       "      <td>Response 1: I've been watching Arsenal games h...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Woah really how is Arsenal doing right now then?</td>\n",
       "      <td>Response 1: They are currently showing great p...</td>\n",
       "      <td>[{\"Roydon\": \"I couldn't agree more! Aubameyang...</td>\n",
       "      <td>Response 1: Arsenal is doing well, did you cat...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nice what breed is your new pet dog?</td>\n",
       "      <td>Response 1: He's a golden retriever, and he's ...</td>\n",
       "      <td>[{\"Roydon\": \"Guess what, I just got a new pet ...</td>\n",
       "      <td>Response 1: He is a golden retriever, and he's...</td>\n",
       "      <td>0.941946</td>\n",
       "      <td>0.741190</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So what you planning to do with your pet dog?</td>\n",
       "      <td>Response 1: I want to teach him some tricks li...</td>\n",
       "      <td>[{\"Roydon\": \"Guess what, I just got a new pet ...</td>\n",
       "      <td>Response 1: I'm planning to take him on walks ...</td>\n",
       "      <td>0.915949</td>\n",
       "      <td>0.684462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question  \\\n",
       "0                  What have you been up to Roydon?   \n",
       "1  Woah really how is Arsenal doing right now then?   \n",
       "2              Nice what breed is your new pet dog?   \n",
       "3     So what you planning to do with your pet dog?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Response 1: \"I've been keeping busy with work ...   \n",
       "1  Response 1: They are currently showing great p...   \n",
       "2  Response 1: He's a golden retriever, and he's ...   \n",
       "3  Response 1: I want to teach him some tricks li...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [{\"Roydon\": \"Hey there! Did you catch the Arse...   \n",
       "1  [{\"Roydon\": \"I couldn't agree more! Aubameyang...   \n",
       "2  [{\"Roydon\": \"Guess what, I just got a new pet ...   \n",
       "3  [{\"Roydon\": \"Guess what, I just got a new pet ...   \n",
       "\n",
       "                                        ground_truth  answer_relevancy  \\\n",
       "0  Response 1: I've been watching Arsenal games h...          0.000000   \n",
       "1  Response 1: Arsenal is doing well, did you cat...          0.000000   \n",
       "2  Response 1: He is a golden retriever, and he's...          0.941946   \n",
       "3  Response 1: I'm planning to take him on walks ...          0.915949   \n",
       "\n",
       "   answer_correctness  context_precision  context_recall  \n",
       "0            0.208525                1.0        0.666667  \n",
       "1            0.909379                1.0        0.333333  \n",
       "2            0.741190                1.0        1.000000  \n",
       "3            0.684462                1.0        0.333333  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-RAG Average Answer Relevancy: 0.21513308049068516\n",
      "Non-RAG Average Answer Correctness: 0.27328692887430783\n",
      "Non-RAG Average Context Precision: 0.0\n",
      "Non-RAG Average Context Recall: 0.0\n",
      "RAG Average Answer Relevancy: 0.4644737698602591\n",
      "RAG Average Answer Correctness: 0.6358889285294633\n",
      "RAG Average Context Precision: 0.9999999999\n",
      "RAG Average Context Recall: 0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate average for non_rag_df\n",
    "non_rag_avg_answer_relevancy = non_rag_df['answer_relevancy'].mean(skipna=True)\n",
    "non_rag_avg_answer_correctness = non_rag_df['answer_correctness'].mean(skipna=True)\n",
    "non_rag_avg_precision = non_rag_df['context_precision'].mean(skipna=True)\n",
    "non_rag_avg_recall = non_rag_df['context_recall'].mean(skipna=True)\n",
    "\n",
    "# Calculate average for rag_df\n",
    "rag_avg_answer_relevancy = rag_df['answer_relevancy'].mean(skipna=True)\n",
    "rag_avg_answer_correctness = rag_df['answer_correctness'].mean(skipna=True)\n",
    "rag_avg_precision = rag_df['context_precision'].mean(skipna=True)\n",
    "rag_avg_recall = rag_df['context_recall'].mean(skipna=True)\n",
    "\n",
    "\n",
    "# Print the averages\n",
    "print(\"Non-RAG Average Answer Relevancy:\", non_rag_avg_answer_relevancy)\n",
    "print(\"Non-RAG Average Answer Correctness:\", non_rag_avg_answer_correctness)\n",
    "print(\"Non-RAG Average Context Precision:\", non_rag_avg_precision)\n",
    "print(\"Non-RAG Average Context Recall:\", non_rag_avg_recall)\n",
    "print(\"RAG Average Answer Relevancy:\", rag_avg_answer_relevancy)\n",
    "print(\"RAG Average Answer Correctness:\", rag_avg_answer_correctness)\n",
    "print(\"RAG Average Context Precision:\", rag_avg_precision)\n",
    "print(\"RAG Average Context Recall:\", rag_avg_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How have you been Roydon?</td>\n",
       "      <td>Response 1: I've been good, thank you for aski...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Response 1: I've been watching Arsenal games h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Woah really how is Arsenal doing right now then?</td>\n",
       "      <td>Response 1: They are currently in a good posit...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Response 1: Arsenal is doing well, did you cat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nice what breed is your new pet dog?</td>\n",
       "      <td>Response 1: He's a golden retriever\\nResponse ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Response 1: He is a golden retriever, and he's...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.832688</td>\n",
       "      <td>0.233893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So what you planning to do with your pet dog?</td>\n",
       "      <td>Response 1: Take him for a walk in the park Re...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Response 1: I'm planning to take him on walks ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860532</td>\n",
       "      <td>0.231712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question  \\\n",
       "0                         How have you been Roydon?   \n",
       "1  Woah really how is Arsenal doing right now then?   \n",
       "2              Nice what breed is your new pet dog?   \n",
       "3     So what you planning to do with your pet dog?   \n",
       "\n",
       "                                              answer contexts  \\\n",
       "0  Response 1: I've been good, thank you for aski...       []   \n",
       "1  Response 1: They are currently in a good posit...       []   \n",
       "2  Response 1: He's a golden retriever\\nResponse ...       []   \n",
       "3  Response 1: Take him for a walk in the park Re...       []   \n",
       "\n",
       "                                        ground_truth  faithfulness  \\\n",
       "0  Response 1: I've been watching Arsenal games h...           NaN   \n",
       "1  Response 1: Arsenal is doing well, did you cat...           0.0   \n",
       "2  Response 1: He is a golden retriever, and he's...           NaN   \n",
       "3  Response 1: I'm planning to take him on walks ...           NaN   \n",
       "\n",
       "   answer_relevancy  answer_correctness  \n",
       "0          0.000000            0.204168  \n",
       "1          0.000000            0.478189  \n",
       "2          0.832688            0.233893  \n",
       "3          0.860532            0.231712  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_rag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path for the Excel file\n",
    "excel_file_path = 'scorings/non_rag_scores.xlsx'\n",
    "\n",
    "# Store the DataFrame into an Excel file\n",
    "non_rag_df.to_excel(excel_file_path)\n",
    "\n",
    "# Specify the file path for the Excel file\n",
    "excel_file_path = 'scorings/rag_scores.xlsx'\n",
    "\n",
    "# Store the DataFrame into an Excel file\n",
    "rag_df.to_excel(excel_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLUERT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'bleurt'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/google-research/bleurt.git\n",
    "!pip install ./bleurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleurt import score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load non rag data from json\n",
    "with open('testing_json/data_sample_non_rag_test_no_history.json', 'r') as json_file:\n",
    "    non_rag_data = json.load(json_file)\n",
    "\n",
    "# Load rag data from json\n",
    "with open('testing_json/data_sample_rag_test_no_history.json', 'r') as json_file:\n",
    "    rag_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint bleurt/bleurt/test_checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint bleurt/bleurt/test_checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:dbleurt_tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:dbleurt_tiny\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7722375392913818, -0.5295215845108032, -0.6895624995231628, -0.43300843238830566]\n"
     ]
    }
   ],
   "source": [
    "# Scores for non-rag\n",
    "checkpoint = \"bleurt/bleurt/test_checkpoint\"\n",
    "references = non_rag_data['ground_truth'] \n",
    "candidates = non_rag_data['answer']\n",
    "\n",
    "scorer = score.BleurtScorer(checkpoint)\n",
    "scores = scorer.score(references=references, candidates=candidates)\n",
    "assert isinstance(scores, list) and len(scores) == 4\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint bleurt/bleurt/test_checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint bleurt/bleurt/test_checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint dbleurt_tiny\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:dbleurt_tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:dbleurt_tiny\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6121180057525635, -0.20042423903942108, 0.3049095869064331, -0.33035808801651]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bleurt/bleurt/test_checkpoint\"\n",
    "references = rag_data['ground_truth'] \n",
    "candidates = rag_data['answer']\n",
    "\n",
    "scorer = score.BleurtScorer(checkpoint)\n",
    "scores = scorer.score(references=references, candidates=candidates)\n",
    "assert isinstance(scores, list) and len(scores) == 4\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: He is a golden retriever, and he's the cutest thing ever! Response 2: He is a golden retriever, and he's so playful! Response 3: He is a golden retriever, and he's so fluffy!\n"
     ]
    }
   ],
   "source": [
    "print(references[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: He's a golden retriever, and he's the cutest thing ever!\n",
      "Response 2: My new dog is a golden retriever, I'm so happy to have him!\n",
      "Response 3: I have a golden retriever, he's adorable and friendly.\n"
     ]
    }
   ],
   "source": [
    "print(candidates[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"Roydon\": \"Guess what, I just got a new pet dog!\", \"Jacob\": \"That\\'s awesome! What breed is it?\"}{\"Roydon\": \"It\\'s a golden retriever, and he\\'s the cutest thing ever!\", \"Jacob\": \"Golden retrievers are so friendly and loyal, you\\'re going to have so much fun with him!\"}{\"Roydon\": \"I couldn\\'t agree more, I feel like my new dog has completed my little family.\", \"Jacob\": \"It\\'s amazing how pets have a way of making a house feel like a home, enjoy every moment with your furry friend!\"}']\n"
     ]
    }
   ],
   "source": [
    "print(rag_data['contexts'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice what breed is your new pet dog?\n"
     ]
    }
   ],
   "source": [
    "print(rag_data['question'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Dataframes\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_metric = GEval(\n",
    "    name=\"Relevance\",\n",
    "    criteria=\"Determine whether the actual output matches the expected output as close as possible.\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the responses generated in 'actual output' are similar to the responses in the 'expected output'\",\n",
    "        \"As long as one of the responses generated is similar to the expected output, the test case is considered correct\",\n",
    "        \"As long as the main content is similar, it is considered okay\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load non rag data from json\n",
    "with open('testing_json/data_sample_non_rag_test_no_history.json', 'r') as json_file:\n",
    "    non_rag_data = json.load(json_file)\n",
    "\n",
    "# Load rag data from json\n",
    "with open('testing_json/data_sample_rag_test_no_history.json', 'r') as json_file:\n",
    "    rag_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6384615545750127\n",
      "One of the responses is similar to the expected output, mentioning activities and interactions with people.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.730660642817738\n",
      "One of the responses generated is similar to the expected output.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7194811378694316\n",
      "Two out of the three responses generated are similar to the expected output.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8782207491967557\n",
      "One of the responses generated (Take him for a walk in the park) is similar to the expected output.\n"
     ]
    }
   ],
   "source": [
    "# Non-rag scores\n",
    "non_rag_scores = []\n",
    "non_rag_reasons = []\n",
    "\n",
    "\n",
    "for i in range(len(non_rag_data['question'])):\n",
    "    test_case = LLMTestCase(\n",
    "        input=non_rag_data['question'][i],\n",
    "        actual_output=non_rag_data['answer'][i],\n",
    "        expected_output=non_rag_data['ground_truth'][i]\n",
    "    )\n",
    "\n",
    "    correctness_metric.measure(test_case)\n",
    "    # print(correctness_metric.score)\n",
    "    # print(correctness_metric.reason)\n",
    "    non_rag_scores.append(correctness_metric.score)\n",
    "    non_rag_reasons.append(correctness_metric.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8361233499701687\n",
      "At least one of the responses in the actual output is similar to the expected output, which is 'I've been keeping busy with work and hanging out with friends.'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9650225847259944\n",
      "Responses are similar to the expected output in terms of discussing Arsenal's current performance and potential.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\roydo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9825289481142185\n",
      "Responses generated are similar to the expected output.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8923881931930074\n",
      "One of the responses generated is similar to the expected output.\n"
     ]
    }
   ],
   "source": [
    "# Non-rag scores\n",
    "rag_scores = []\n",
    "rag_reasons = []\n",
    "\n",
    "\n",
    "for i in range(len(rag_data['question'])):\n",
    "    test_case = LLMTestCase(\n",
    "        input=rag_data['question'][i],\n",
    "        actual_output=rag_data['answer'][i],\n",
    "        expected_output=rag_data['ground_truth'][i]\n",
    "    )\n",
    "\n",
    "    correctness_metric.measure(test_case)\n",
    "    print(correctness_metric.score)\n",
    "    print(correctness_metric.reason)\n",
    "    rag_scores.append(correctness_metric.score)\n",
    "    rag_reasons.append(correctness_metric.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Non-RAG Scores-----------------\n",
      "[0.6384615545750127, 0.730660642817738, 0.7194811378694316, 0.8782207491967557]\n",
      "['One of the responses is similar to the expected output, mentioning activities and interactions with people.', 'One of the responses generated is similar to the expected output.', 'Two out of the three responses generated are similar to the expected output.', 'One of the responses generated (Take him for a walk in the park) is similar to the expected output.']\n",
      "-----------------RAG Scores-----------------\n",
      "[0.8361233499701687, 0.9650225847259944, 0.9825289481142185, 0.8923881931930074]\n",
      "[\"At least one of the responses in the actual output is similar to the expected output, which is 'I've been keeping busy with work and hanging out with friends.'\", \"Responses are similar to the expected output in terms of discussing Arsenal's current performance and potential.\", 'Responses generated are similar to the expected output.', 'One of the responses generated is similar to the expected output.']\n"
     ]
    }
   ],
   "source": [
    "# Printing out scores\n",
    "print(\"-----------------Non-RAG Scores-----------------\")\n",
    "print(non_rag_scores)\n",
    "print(non_rag_reasons)\n",
    "\n",
    "print(\"-----------------RAG Scores-----------------\")\n",
    "print(rag_scores)\n",
    "print(rag_reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Scores                                            Reasons\n",
      "0  0.638462  One of the responses is similar to the expecte...\n",
      "1  0.730661  One of the responses generated is similar to t...\n",
      "2  0.719481  Two out of the three responses generated are s...\n",
      "3  0.878221  One of the responses generated (Take him for a...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine scores and reasons into a DataFrame\n",
    "rag_df = pd.DataFrame({'Scores': rag_scores, 'Reasons': rag_reasons})\n",
    "non_rag_df = pd.DataFrame({'Scores': non_rag_scores, 'Reasons': non_rag_reasons})\n",
    "\n",
    "# Print the DataFrame\n",
    "#print(rag_df)\n",
    "print(non_rag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path for the Excel file\n",
    "excel_file_path = 'scorings/g_eval_non_rag_scores.xlsx'\n",
    "\n",
    "# Store the DataFrame into an Excel file\n",
    "non_rag_df.to_excel(excel_file_path)\n",
    "\n",
    "# Specify the file path for the Excel file\n",
    "excel_file_path = 'scorings/g_eval_rag_scores.xlsx'\n",
    "\n",
    "# Store the DataFrame into an Excel file\n",
    "rag_df.to_excel(excel_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
